{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "checkpoint_path = Path('./checkpoints') / \"mlp\"\n",
    "input_path = Path('../data/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeebcc054967169f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def seed_torch(seed_value):\n",
    "    random.seed(seed_value)  # Python\n",
    "    np.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars    \n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed_value)\n",
    "\n",
    "    torch.cuda.amp.GradScaler.enabled = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d49bfd7747d070b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LOW_CITY_THR = 9\n",
    "LAGS = 5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b07adb22f6f4951"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read CSV using Pandas\n",
    "raw = pd.read_csv(input_path / 'train_and_test_2.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d84feaf10702f086"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace 0s in 'city_id' with NaN\n",
    "raw.loc[raw['city_id'] == 0, 'city_id'] = np.NaN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73ebe6a6259dbe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group by 'city_id' and count 'utrip_id'\n",
    "df = raw[(raw.istest == 0) | (raw.icount > 0)].groupby('city_id')['utrip_id'].count().reset_index()\n",
    "df.columns = ['city_id', 'city_count']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd0378443314cee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw = raw.merge(df, how='left', on='city_id')\n",
    "raw.loc[raw.city_count <= LOW_CITY_THR, 'city_id'] = -1\n",
    "raw = raw.sort_values(['utrip_id', 'checkin'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ee0955baa4c8742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Factorize categorical columns\n",
    "CATS = ['city_id', 'hotel_country', 'booker_country', 'device_class']\n",
    "MAPS = []\n",
    "for c in CATS:\n",
    "    raw[c + '_'], mp = raw[c].factorize()\n",
    "    MAPS.append(mp)\n",
    "    print('created', c + '_')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55fc612c3fe12547"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the index of the \"low city\" (-1)\n",
    "LOW_CITY = np.where(MAPS[0] == -1)[0][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2418639d80c358fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of unique categories for one-hot encoding\n",
    "NUM_CITIES = raw.city_id_.max() + 1\n",
    "NUM_HOTELS = raw.hotel_country_.max() + 1\n",
    "NUM_DEVICE = raw.device_class_.max() + 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e0a803a42006b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reverse the data for training set\n",
    "raw['reverse'] = 0\n",
    "rev_raw = raw[raw.istest == 0].copy()\n",
    "rev_raw['reverse'] = 1\n",
    "rev_raw['utrip_id'] = rev_raw['utrip_id'] + '_r'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c255a21f23622424"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = rev_raw['icount'].values.copy()\n",
    "rev_raw['icount'] = rev_raw['dcount']\n",
    "rev_raw['dcount'] = tmp\n",
    "rev_raw = rev_raw.sort_values(['utrip_id', 'dcount']).reset_index(drop=True)\n",
    "raw = pd.concat([raw, rev_raw]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e86fd6764acd56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add 'sorting' column\n",
    "raw['sorting'] = np.arange(raw.shape[0])\n",
    "\n",
    "# Factorize 'utrip_id'\n",
    "raw['utrip_id' + '_'], mp = raw['utrip_id'].factorize()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78a9f87a20a7d88c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to engineer lag features\n",
    "def shift_feature(df, group_col, feature_col, shift_by, num_categories, new_col_name):\n",
    "    df[new_col_name] = df.groupby(group_col)[feature_col].shift(shift_by, fill_value=num_categories)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3ae4a85111a867"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Engineer lag features\n",
    "lag_cities = []\n",
    "lag_countries = []\n",
    "for i in range(1, LAGS + 1):\n",
    "    raw[f'city_id_lag{i}'] = raw.groupby('utrip_id_')['city_id_'].shift(i, fill_value=NUM_CITIES)\n",
    "    lag_cities.append(f'city_id_lag{i}')\n",
    "    raw[f'country_lag{i}'] = raw.groupby('utrip_id_')['hotel_country_'].shift(i, fill_value=NUM_CITIES)\n",
    "    lag_countries.append(f'country_lag{i}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9d2939ebd83b42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract the first city and country for each trip\n",
    "tmpD = raw[raw['dcount'] == 0][['utrip_id', 'city_id_']]\n",
    "tmpD.columns = ['utrip_id', 'first_city']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')\n",
    "tmpD = raw[raw['dcount'] == 0][['utrip_id', 'hotel_country_']]\n",
    "tmpD.columns = ['utrip_id', 'first_country']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a572a77ce27bf0b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'checkin' and 'checkout' columns to datetime in Pandas\n",
    "raw['checkin'] = pd.to_datetime(raw['checkin'], format=\"%Y-%m-%d\")\n",
    "raw['checkout'] = pd.to_datetime(raw['checkout'], format=\"%Y-%m-%d\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b978432dd173cdf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract month, weekday for checkin and checkout, and calculate trip length in Pandas\n",
    "raw['mn'] = raw['checkin'].dt.month\n",
    "raw['dy1'] = raw['checkin'].dt.weekday\n",
    "raw['dy2'] = raw['checkout'].dt.weekday\n",
    "raw['length'] = np.log1p((raw['checkout'] - raw['checkin']).dt.days)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd25fc7f3ed059db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract first checkin and last checkout for each trip in Pandas\n",
    "tmpD = raw[raw['dcount'] == 0][['utrip_id', 'checkin']]\n",
    "tmpD.columns = ['utrip_id', 'first_checkin']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')\n",
    "tmpD = raw[raw['icount'] == 0][['utrip_id', 'checkout']]\n",
    "tmpD.columns = ['utrip_id', 'last_checkout']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9645267774b292f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate trip length and derive last checkin and first checkout in Pandas\n",
    "raw['trip_length'] = ((raw['last_checkout'] - raw['first_checkin']).dt.days)\n",
    "raw['trip_length'] = np.log1p(np.abs(raw['trip_length'])) * np.sign(raw['trip_length'])\n",
    "tmpD = raw[raw['icount'] == 0][['utrip_id', 'checkin']]\n",
    "tmpD.columns = ['utrip_id', 'last_checkin']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')\n",
    "tmpD = raw[raw['dcount'] == 0][['utrip_id', 'checkout']]\n",
    "tmpD.columns = ['utrip_id', 'first_checkout']\n",
    "raw = raw.merge(tmpD, on='utrip_id', how='left')\n",
    "raw['trip_length'] = raw['trip_length'] - raw['trip_length'].mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "741022e704a6a0e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Engineer checkout lag and calculate lapse in Pandas\n",
    "raw['checkout_lag1'] = raw.groupby('utrip_id_')['checkout'].shift(1, fill_value=None)\n",
    "raw['lapse'] = (raw['checkin'] - raw['checkout_lag1']).dt.days.fillna(-1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b72085198189f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Engineer weekend and season features in Pandas\n",
    "raw['day_name'] = raw['checkin'].dt.weekday\n",
    "raw['weekend'] = raw['day_name'].isin([5, 6]).astype('int8')\n",
    "df_season = pd.DataFrame({'mn': range(1, 13), 'season': ([0] * 3) + ([1] * 3) + ([2] * 3) + ([3] * 3)})\n",
    "raw = raw.merge(df_season, how='left', on='mn')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f7a87108843cf2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a305688f00395fa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['lapse'], bins=100, log=True)\n",
    "raw['lapse'].mean(), raw['lapse'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "126be4c77f72c8bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['N'], bins=100, log=True)\n",
    "raw['N'].mean(), raw['N'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3556658d8b09b2cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw['N'] = raw['N'] - raw['N'].mean()\n",
    "raw['N'] /= 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a786fad85364de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['trip_length'], bins=100, log=True)\n",
    "raw['trip_length'].mean(), raw['length'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "807b0e03bceaa5dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['length'], bins=100, log=True)\n",
    "raw['length'].mean(), raw['length'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "416f7abe7098ca44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw['log_icount'] = np.log1p(raw['icount'])\n",
    "raw['log_dcount'] = np.log1p(raw['dcount'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfbc9626a4b764cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['log_icount'], bins=100, log=True)\n",
    "raw['log_icount'].mean(), raw['log_icount'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f62a29e9bea10d83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['log_dcount'], bins=100, log=True)\n",
    "raw['log_dcount'].mean(), raw['log_dcount'].std()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a3e7bdad2910386"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw['mn'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a49e81559cc3bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw['dy1'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336400ad2e44310d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw['dy2'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b6fe359b377da40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BookingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target=None,\n",
    "                 ):\n",
    "        super(BookingDataset, self).__init__()\n",
    "        self.lag_cities_ = data[lag_cities].values\n",
    "        self.mn = data['mn'].values - 1\n",
    "        self.dy1 = data['dy1'].values\n",
    "        self.dy2 = data['dy2'].values\n",
    "        self.length = data['length'].values\n",
    "        self.trip_length = data['trip_length'].values\n",
    "        self.N = data['N'].values\n",
    "        self.log_icount = data['log_icount'].values\n",
    "        self.log_dcount = data['log_dcount'].values\n",
    "        self.lag_countries_ = data[lag_countries].values\n",
    "        self.first_city = data['first_city'].values\n",
    "        self.first_country = data['first_country'].values\n",
    "        self.booker_country_ = data['booker_country_'].values\n",
    "        self.device_class_ = data['device_class_'].values\n",
    "        self.lapse = data['lapse'].values\n",
    "        self.season = data['season'].values\n",
    "        self.weekend = data['weekend'].values\n",
    "        if target is None:\n",
    "            self.target = None\n",
    "        else:\n",
    "            self.target = data[target].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lag_cities_)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        input_dict = {\n",
    "            'lag_cities_': torch.tensor(self.lag_cities_[idx], dtype=torch.long),\n",
    "            'mn': torch.tensor([self.mn[idx]], dtype=torch.long),\n",
    "            'dy1': torch.tensor([self.dy1[idx]], dtype=torch.long),\n",
    "            'dy2': torch.tensor([self.dy2[idx]], dtype=torch.long),\n",
    "            'length': torch.tensor([self.length[idx]], dtype=torch.float),\n",
    "            'trip_length': torch.tensor([self.trip_length[idx]], dtype=torch.float),\n",
    "            'N': torch.tensor([self.N[idx]], dtype=torch.float),\n",
    "            'log_icount': torch.tensor([self.log_icount[idx]], dtype=torch.float),\n",
    "            'log_dcount': torch.tensor([self.log_dcount[idx]], dtype=torch.float),\n",
    "            'lag_countries_': torch.tensor(self.lag_countries_[idx], dtype=torch.long),\n",
    "            'first_city': torch.tensor([self.first_city[idx]], dtype=torch.long),\n",
    "            'first_country': torch.tensor([self.first_country[idx]], dtype=torch.long),\n",
    "            'booker_country_': torch.tensor([self.booker_country_[idx]], dtype=torch.long),\n",
    "            'device_class_': torch.tensor([self.device_class_[idx]], dtype=torch.long),\n",
    "            'lapse': torch.tensor([self.lapse[idx]], dtype=torch.float),\n",
    "            'season': torch.tensor([self.season[idx]], dtype=torch.long),\n",
    "            'weekend': torch.tensor([self.weekend[idx]], dtype=torch.long),\n",
    "        }\n",
    "        if self.target is not None:\n",
    "            input_dict['target'] = torch.tensor([self.target[idx]], dtype=torch.long)\n",
    "        return input_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6affc2cf57b37356"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = BookingDataset(raw, 'city_id_')\n",
    "\n",
    "dataset.__getitem__(3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d2eeefcbecdbb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer, scheduler, scaler, device):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    train_loss = []\n",
    "    bar = tqdm(range(len(loader)))\n",
    "    load_iter = iter(loader)\n",
    "    batch = load_iter.next()\n",
    "    batch = {k: batch[k].to(device, non_blocking=True) for k in batch.keys()}\n",
    "\n",
    "    for i in bar:\n",
    "\n",
    "        old_batch = batch\n",
    "        if i + 1 < len(loader):\n",
    "            batch = load_iter.next()\n",
    "            batch = {k: batch[k].to(device, non_blocking=True) for k in batch.keys()}\n",
    "\n",
    "        out_dict = model(old_batch)\n",
    "        logits = out_dict['logits']\n",
    "        loss = out_dict['loss']\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(loader, model, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    TARGETS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(range(len(loader)))\n",
    "        load_iter = iter(loader)\n",
    "        batch = load_iter.next()\n",
    "        batch = {k: batch[k].to(device, non_blocking=True) for k in batch.keys()}\n",
    "\n",
    "        for i in bar:\n",
    "\n",
    "            old_batch = batch\n",
    "            if i + 1 < len(loader):\n",
    "                batch = load_iter.next()\n",
    "                batch = {k: batch[k].to(device, non_blocking=True) for k in batch.keys()}\n",
    "\n",
    "            out_dict = model(old_batch)\n",
    "            logits = out_dict['logits']\n",
    "            loss = out_dict['loss']\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "            target = old_batch['target']\n",
    "            LOGITS.append(logits.detach())\n",
    "            TARGETS.append(target.detach())\n",
    "            val_loss.append(loss_np)\n",
    "\n",
    "            smooth_loss = sum(val_loss[-100:]) / min(len(val_loss), 100)\n",
    "            bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "    LOGITS = torch.cat(LOGITS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "\n",
    "    return val_loss, LOGITS, TARGETS\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7b6411cf6d5af86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, scaler, best_score, fold, seed, fname):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'scaler': scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'best_score': best_score,\n",
    "    }\n",
    "    torch.save(checkpoint, f'./checkpoints/{fname}/{fname}_{fold}_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "393e29d8d39a7199"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_checkpoint(fold, seed, device, fname):\n",
    "    model = Net(NUM_CITIES + 1, NUM_HOTELS + 1, EMBEDDING_DIM, HIDDEN_DIM, dropout_rate=DROPOUT_RATE,\n",
    "                loss=False).to(device)\n",
    "\n",
    "    checkpoint = torch.load(f'./checkpoints/{fname}/{fname}_{fold}_{seed}.pt')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a97578fe801a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=LOW_CITY)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_cities, num_countries, embedding_dim, hidden_dim, dropout_rate, loss=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.loss = loss\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.cities_embeddings = nn.Embedding(num_cities, embedding_dim)\n",
    "        self.cities_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('city embedding data shape', self.cities_embeddings.weight.shape)\n",
    "\n",
    "        self.countries_embeddings = nn.Embedding(num_countries, embedding_dim)\n",
    "        self.countries_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('country embedding data shape', self.countries_embeddings.weight.shape)\n",
    "\n",
    "        self.mn_embeddings = nn.Embedding(12, embedding_dim)\n",
    "        self.mn_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.dy1_embeddings = nn.Embedding(7, embedding_dim)\n",
    "        self.dy1_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.dy2_embeddings = nn.Embedding(7, embedding_dim)\n",
    "        self.dy2_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        #self.season_embeddings = nn.Embedding(7, embedding_dim)     \n",
    "        #self.season_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.weekend_embeddings = nn.Embedding(2, embedding_dim)\n",
    "        self.weekend_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.linear_length = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_length = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_length = nn.ReLU()\n",
    "\n",
    "        self.linear_trip_length = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_trip_length = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_trip_length = nn.ReLU()\n",
    "\n",
    "        self.linear_N = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_N = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_N = nn.ReLU()\n",
    "\n",
    "        self.linear_log_icount = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_log_icount = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_log_icount = nn.ReLU()\n",
    "\n",
    "        self.linear_log_dcount = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_log_dcount = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_log_dcount = nn.ReLU()\n",
    "\n",
    "        self.devices_embeddings = nn.Embedding(NUM_DEVICE, embedding_dim)\n",
    "        self.devices_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('device_embeddings data shape', self.devices_embeddings.weight.shape)\n",
    "\n",
    "        self.linear_lapse = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_lapse = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_lapse = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear((len(lag_cities) + len(lag_countries) + 1) * embedding_dim, hidden_dim)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activate1 = nn.PReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activate2 = nn.PReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.linear3 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.norm3 = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate3 = nn.PReLU()\n",
    "        self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "        self.output_layer_bias = nn.Parameter(torch.Tensor(num_cities, ))\n",
    "        self.output_layer_bias.data.normal_(0., 0.01)\n",
    "\n",
    "    def get_embed(self, x, embed):\n",
    "        bs = x.shape[0]\n",
    "        x = embed(x)\n",
    "        # lag_embed.shape: bs, x.shape[1], embedding_dim\n",
    "        x = x.view(bs, -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        lag_embed = self.get_embed(input_dict['lag_cities_'], self.cities_embeddings)\n",
    "        lag_countries_embed = self.get_embed(input_dict['lag_countries_'], self.countries_embeddings)\n",
    "        mn_embed = self.get_embed(input_dict['mn'], self.mn_embeddings)\n",
    "        dy1_embed = self.get_embed(input_dict['dy1'], self.dy1_embeddings)\n",
    "        dy2_embed = self.get_embed(input_dict['dy2'], self.dy2_embeddings)\n",
    "        #season_embed = self.get_embed(input_dict['season'], self.season_embeddings)  \n",
    "        weekend_embed = self.get_embed(input_dict['weekend'], self.weekend_embeddings)\n",
    "        length = input_dict['length']\n",
    "        length_embed = self.activate_length(self.norm_length(self.linear_length(length)))\n",
    "        trip_length = input_dict['trip_length']\n",
    "        trip_length_embed = self.activate_trip_length(self.norm_trip_length(self.linear_trip_length(trip_length)))\n",
    "        N = input_dict['N']\n",
    "        N_embed = self.activate_N(self.norm_N(self.linear_N(N)))\n",
    "        lapse = input_dict['lapse']\n",
    "        lapse_embed = self.activate_lapse(self.norm_lapse(self.linear_lapse(lapse)))\n",
    "        log_icount = input_dict['log_icount']\n",
    "        log_icount_embed = self.activate_log_icount(self.norm_log_icount(self.linear_log_icount(log_icount)))\n",
    "        log_dcount = input_dict['length']\n",
    "        log_dcount_embed = self.activate_log_dcount(self.norm_log_dcount(self.linear_log_dcount(log_dcount)))\n",
    "        first_city_embed = self.get_embed(input_dict['first_city'], self.cities_embeddings)\n",
    "        first_country_embed = self.get_embed(input_dict['first_country'], self.countries_embeddings)\n",
    "        booker_country_embed = self.get_embed(input_dict['booker_country_'], self.countries_embeddings)\n",
    "        device_embed = self.get_embed(input_dict['device_class_'], self.devices_embeddings)\n",
    "        x = (mn_embed + dy1_embed + dy2_embed + length_embed + log_icount_embed + log_dcount_embed \\\n",
    "             + first_city_embed + first_country_embed + booker_country_embed + device_embed \\\n",
    "             + trip_length_embed + N_embed + lapse_embed + weekend_embed)\n",
    "        x = torch.cat([lag_embed, lag_countries_embed, x], -1)\n",
    "        x = self.activate1(self.norm1(self.linear1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x + self.activate2(self.norm2(self.linear2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.activate3(self.norm3(self.linear3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        logits = F.linear(x, self.cities_embeddings.weight, bias=self.output_layer_bias)\n",
    "        output_dict = {\n",
    "            'logits': logits\n",
    "        }\n",
    "        if self.loss:\n",
    "            target = input_dict['target'].squeeze(1)\n",
    "            #print(logits.shape, target.shape)\n",
    "            loss = loss_fct(logits, target)\n",
    "            output_dict['loss'] = loss\n",
    "        return output_dict\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1dac141cccac71b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1024\n",
    "WORKERS = 8\n",
    "LR = 1e-3\n",
    "EPOCHS = 12\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 1024\n",
    "DROPOUT_RATE = 0.2\n",
    "device = torch.device('mps')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af22f0fda93ae805"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top4(preds):\n",
    "    TOP4 = np.empty((preds.shape[0], 4))\n",
    "    for i in range(4):\n",
    "        x = np.argmax(preds, axis=1)\n",
    "        TOP4[:, i] = x\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        np.put_along_axis(preds, x, -1e10, axis=1)\n",
    "    return TOP4\n",
    "\n",
    "\n",
    "def top4(preds, target):\n",
    "    TOP4 = get_top4(preds)\n",
    "    acc = np.max(TOP4 == target, axis=1)\n",
    "    acc = np.mean(acc)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b995da14fc3f886d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_WITH_TEST = True\n",
    "\n",
    "seed = 0\n",
    "seed_torch(seed)\n",
    "\n",
    "preds_all = []\n",
    "best_scores = []\n",
    "best_epochs = []\n",
    "for fold in range(5):\n",
    "\n",
    "    seed_torch(seed)\n",
    "    preds_fold = []\n",
    "    print('#' * 25)\n",
    "    print(f'### FOLD {fold}')\n",
    "    if TRAIN_WITH_TEST:\n",
    "        train = raw.loc[\n",
    "            (raw.fold != fold) & (raw.dcount > 0) & (raw.istest == 0) | ((raw.istest == 1) & (raw.icount > 0))].copy()\n",
    "    else:\n",
    "        train = raw.loc[(raw.fold != fold) & (raw.dcount > 0) & (raw.istest == 0)].copy()\n",
    "    valid = raw.loc[(raw.fold == fold) & (raw.istest == 0) & (raw.icount == 0) & (raw.reverse == 0)].copy()\n",
    "    print(train.shape, valid.shape)\n",
    "\n",
    "    train_dataset = BookingDataset(train, target='city_id_')\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    valid_dataset = BookingDataset(valid, target='city_id_')\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = Net(NUM_CITIES + 1, NUM_HOTELS + 1, EMBEDDING_DIM, HIDDEN_DIM, dropout_rate=DROPOUT_RATE).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                    pct_start=0.1,\n",
    "                                                    div_factor=1e3,\n",
    "                                                    max_lr=3e-3,\n",
    "                                                    epochs=EPOCHS,\n",
    "                                                    steps_per_epoch=int(\n",
    "                                                        np.ceil(len(train_data_loader) / GRADIENT_ACCUMULATION)))\n",
    "    scaler = \"\"\n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(time.ctime(), 'Epoch:', epoch, flush=True)\n",
    "        train_loss = train_epoch(train_data_loader, model, optimizer, scheduler, scaler, device)\n",
    "        val_loss, PREDS, TARGETS = val_epoch(valid_data_loader, model, device)\n",
    "        PREDS[:, LOW_CITY] = -1e10  # remove low frequency cities\n",
    "        score = top4(PREDS, TARGETS)\n",
    "\n",
    "        print(\n",
    "            f'Fold {fold} Seed {seed} Ep {epoch} lr {optimizer.param_groups[0][\"lr\"]:.7f} train loss {np.mean(train_loss):4f} val loss {np.mean(val_loss):4f} score {score:4f}',\n",
    "            flush=True)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_epoch = epoch\n",
    "            preds_fold = PREDS\n",
    "            save_checkpoint(model, optimizer, scaler, scheduler, best_score, fold, seed, fname)\n",
    "    del model, scaler, scheduler, optimizer, valid_data_loader, valid_dataset, train_data_loader, train_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    preds_all.append(preds_fold)\n",
    "    print(f'fold {fold}, best score: {best_score:.6f} best epoch: {best_epoch:3d}')\n",
    "    best_scores.append(best_score)\n",
    "    best_epochs.append(best_epoch)\n",
    "    #with open('../checkpoints/%s/%s_%d_preds.pkl' % (fname, fname, seed), 'wb') as file:\n",
    "    #    pkl.dump(preds_all, file)\n",
    "\n",
    "    #break\n",
    "    print()\n",
    "    for fold, (best_score, best_epoch) in enumerate(zip(best_scores, best_epochs)):\n",
    "        print(f'fold {fold}, best score: {best_score:.6f} best epoch: {best_epoch:3d}')\n",
    "    print(f'seed {seed} best score: {best_score:.6f} best epoch: {best_epochs:.1f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af8042bf4e7c73e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
